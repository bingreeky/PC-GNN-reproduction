import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
from torch_geometric.data import HeteroData


from typing import List
from tqdm import tqdm
from collections import defaultdict
from operator import itemgetter

from myutils import LabelBalancedSampler
from agg import InterAgg


class PCGNN(nn.Module):
    """ 
    ä¸€å±‚PC-GNNç”¨ä»¥message passing -> æ ¸å¿ƒç‰¹ç‚¹æ˜¯ï¼Œå±…ç„¶éœ€è¦label=ã€‚= \\
    è®ºæ–‡æºä»£ç å±…ç„¶ç›´æ¥æŠŠä¸€å±‚å½“æ•´ä¸ªæ¨¡å‹äº†â€¦â€¦å¥½ğŸ•å•Š
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        # adj_lists: List[defaultdict],
        # train_pos_mask: list,
        device: torch.device,
        normalize=True,
        num_classes: int = 2,
        bias=False,
    ):
        """
        ä¸€å±‚Pick&Choose çš„ message passager \\
        éš¾é“interå±‚ä¸åº”è¯¥åœ¨è¿™é‡Œè¿›è¡Œå£°æ˜å—â€¦â€¦ \\
        :param in_channels: è¾“å…¥çš„ç‰¹å¾ç»´æ•°
        :param out_channels: è¾“å‡ºçš„ç‰¹å¾ç»´æ•°
        :param num_classes: æœ€åéœ€è¦åšèŠ‚ç‚¹åˆ†ç±»çš„ç±»æ•°
        """
        super(PCGNN, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalize = normalize
        self.interAgg = InterAgg(
            feature_dim=in_channels,
            output_dim=out_channels,
            # adj_lists=adj_lists,
            # train_pos_mask=train_pos_mask,
            device=device
        )

        self.reset_parameters()

    def reset_parameters(self):
        # self.lin_l.reset_parameters()
        # self.lin_r.reset_parameters()
        pass

    def forward(self, features, labels, batch_mask, train_pos_mask, adj_lists, train_flag=True):
        """
        :param features: (|N|, input_channels)
        :param labels: (|N|,)
        :param batch_mask: (|B|,)åœ¨æ­¤æ¬¡è¿‡ç¨‹ä¸­éœ€è¦è€ƒå¯Ÿçš„ä¸­å¿ƒç‚¹çš„mask
        :param train_pos_mask:
        :param adj_lists:
        :return output_embeds: (|B|,out_dim)
        :return label_scores: (|B|,2)
        """

        embeds, logits = self.interAgg(
            features=features,
            batch_center_mask=batch_mask,
            batch_center_label=labels[batch_mask],
            train_pos_mask=train_pos_mask,
            adj_lists=adj_lists
        )

        return embeds, logits


class GNNStack(torch.nn.Module):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        device: torch.device,
        num_classes: int = 2,
        num_layers: int = 1,
        dropout: float = 0.5,
        heads: int = 1,
        model_type: str = 'PCGNN',
        emb: bool = False
    ) -> None:
        super(GNNStack, self).__init__()

        conv_model = self.build_conv_model(model_type)
        # self.convs = nn.ModuleList()
        # self.convs.append(conv_model(
        #     in_channels=input_dim,
        #     out_channels=hidden_dim
        # ))
        assert (num_layers >= 1), 'Number of layers is not >=1'
        if num_layers == 1:
            self.convs = conv_model(
                in_channels=input_dim,
                out_channels=output_dim,
                device=device
            )
        else:
            # for l in range(num_layers):
            #     if l == 0:
            #         self.convs.append(conv_model(input_dim,hidden_dim))
            #     elif l == num_layers-1:
            #         self.convs.append(conv_model(heads*hidden_dim,output_dim))
            #     else:
            #         self.convs.append(conv_model(heads*hidden_dim, hidden_dim))
            raise NotImplementedError

        # # post-message-passing
        # self.post_mp = nn.Sequential(
        #     nn.Linear(heads * hidden_dim, hidden_dim), nn.Dropout(self.dropout),
        #     nn.Linear(hidden_dim, output_dim))
        # # å—¯â€¦â€¦å…¶å®æˆ‘ä¸€å…±è®¾äº†num_layers+1å±‚

        # PCGNNæœ€åè¿˜æœ‰ä¸€ä¸ªçº¿æ€§å±‚ç”¨äºnodeåˆ†ç±»ï¼
        self.final_proj = nn.Linear(output_dim, num_classes)

        self.dropout = dropout
        self.num_layers = num_layers

        self.emb = emb
        self.criterion = nn.CrossEntropyLoss()

    def build_conv_model(self, model_type):
        if model_type == 'PCGNN':
            return PCGNN
        else:
            raise NotImplementedError

    def forward(self, features, labels, batch_mask, train_pos_mask, adj_lists):
        """ 
        :param features: æ‰€æœ‰ç‚¹çš„features
        :param labels: æ‰€æœ‰ç‚¹çš„label
        :param batch_mask: æœ¬æ‰¹æ¬¡ç‚¹çš„mask æˆ–è€…æ˜¯ä¹‹åtest/validçš„æ—¶å€™çš„mask
        :param train_pos_mask:
        :param adj_lists:
        """
        # x, edge_index, batch = data.x, data.edge_index, data.batch

        for i in range(self.num_layers):
            embeds, logits = self.convs(
                features, labels, batch_mask, train_pos_mask, adj_lists)
            # x = F.relu(x)
            # x = F.dropout(x, p=self.dropout,training=self.training)

        # x = self.post_mp(x)
        if self.emb == True:
            return embeds

        return embeds, logits

    def loss(self, features, labels, batch_mask, train_pos_mask, adj_lists):
        """ 
        PCGNNçš„lossåŒ…æ‹¬ä¸¤ä¸ª:loss_{gnn}å’Œloss_{dist}
        """
        embeds, logits = self.forward(
            features=features,
            labels=labels,
            batch_mask=batch_mask,
            train_pos_mask=train_pos_mask,
            adj_lists=adj_lists
        )

        # PCGNNæœ‰ä¸¤ä¸ªloss
        # loss_{gnn}
        gnn_pred = self.final_proj(embeds)
        gnn_loss = self.criterion(gnn_pred, labels[batch_mask].squeeze())

        # loss_{dist}
        dist_loss = self.criterion(logits, labels[batch_mask].squeeze())

        return gnn_loss + dist_loss


class ModelHandler():

    def __init__(
        self,
        data: HeteroData,
        data_name: str = 'Amazon',
        random_seed: int = 42,
        use_cuda: bool = True,
        opt: str = 'adam',
        weight_decay: float = 5e-3,
        lr: float = 0.01,
        dropout: float = 0.5,
        num_layers: int = 2,
        num_epochs: int = 50,
        batch_size: int = 1024
    ) -> None:
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)

        # preprare data
        self.data = data
        # å…³äºæ­¤æ•°æ®é›†æœ‰ä¸ªå¾ˆé‡è¦çš„äº‹ï¼Œå³ï¼Œnodeæ•°æ®éƒ½æ˜¯ndarrayæ ¼å¼ï¼Œrelationçš„æ•°æ®éƒ½æ˜¯tensorï¼
        # TODO normlize feature??

        if use_cuda and torch.cuda.is_available():
            self.device = torch.device('cuda:0')
        else:
            self.device = torch.device('cpu')

        self.opt, self.weight_decay, self.lr = opt, weight_decay, lr
        self.num_epochs = num_epochs
        self.batch_size = batch_size

    def train(self):
        feat_data, label_data = self.data['review'].x, self.data['review'].y
        train_mask, valid_mask, test_mask = self.data['review'].train_mask, self.data[
            'review'].valid_mask, self.data['review'].test_mask
        train_pos_mask = self.data['review'].train_pos_mask
        adj_lists = [
            self.data['review', 'r1', 'review'].adj_list[0],
            self.data['review', 'r2', 'review'].adj_list[0],
            self.data['review', 'r3', 'review'].adj_list[0],
        ]

        features = nn.Embedding(feat_data.shape[0], feat_data.shape[1])
        features.weight = nn.Parameter(torch.FloatTensor(
            feat_data), requires_grad=False).to(self.device)

        feat_data = torch.FloatTensor(feat_data, device=self.device)
        label_data = torch.LongTensor(label_data, device=self.device)

        # æˆ‘ä»¬åªä½¿ç”¨PCGNN
        GNN = GNNStack(
            input_dim=feat_data.shape[1],
            hidden_dim=64,
            output_dim=64,
            device=self.device
        )

        # optimizer
        if(self.opt == 'adam'):
            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, GNN.parameters(
            )), lr=self.lr, weight_decay=self.weight_decay)
        else:
            raise NotImplementedError("This optimizer is not implemented yet.")

        for epoch in range(self.num_epochs):
            # Pické˜¶æ®µï¼Œå€ŸåŠ©labelBalancedSamplerè¿›è¡Œä¸€ä¸ªé™é‡‡æ ·
            train_mask = LabelBalancedSampler(
                train_mask, label_data[train_mask], self.data['homo_adj_list'][0], len(self.data['review'].train_pos_mask)*2)
            # è¿™é‡Œçš„train maskæ˜¯ç»è¿‡æ¦‚ç‡pickè¿‡çš„ï¼

            # æˆ‘ä»¬åœ¨å‡†å¤‡æ•°æ®é›†æ—¶å·²ç»å‡†å¤‡å¥½äº†train_pos/neg_mask
            num_batches = int(len(train_mask) / self.batch_size) + 1

            loss = 0.

            # å¼€å§‹batchè®­ç»ƒ
            for batch in tqdm(range(num_batches)):
                ind_start = batch*self.batch_size
                ind_end = min((batch+1)*self.batch_size, len(train_mask))
                batch_nodes_mask = train_mask[ind_start:ind_end]
                # batch_label = label_data[batch_nodes_mask]
                # TODO è¿™é‡Œçš„ç±»å‹å¯èƒ½å­˜åœ¨é—®é¢˜

                optimizer.zero_grad()
                loss = GNN.loss(
                    features=feat_data,
                    labels=label_data,
                    batch_mask=batch_nodes_mask,
                    train_pos_mask=train_pos_mask,
                    adj_lists=adj_lists
                )

                loss.backward()
                optimizer.step()
                loss += loss.item()

                print(f'Epoch: {epoch}, loss: {loss.item() / num_batches}')
