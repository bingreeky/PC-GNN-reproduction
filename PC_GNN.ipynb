{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_scatter\n",
    "import torch_sparse\n",
    "import torch_cluster\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from myutils import constructPyGHeteroData\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "数据集信息：\n",
    "\n",
    "https://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, normalize=True,\n",
    "                 bias=False, **kwargs):\n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here!\n",
    "        # Define the layers needed for the message and update functions below.\n",
    "        # self.lin_l is the linear transformation that you apply to embedding\n",
    "        #            for central node.\n",
    "        # self.lin_r is the linear transformation that you apply to aggregated\n",
    "        #            message from neighbors.\n",
    "        # Don't forget the bias!\n",
    "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        self.lin_l = nn.Linear(in_channels, out_channels, bias=bias)\n",
    "        self.lin_r = nn.Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        out = self.propagate(edge_index=edge_index, x=(x, x), size=size)\n",
    "        out += self.lin_l(x)\n",
    "        if self.normalize: out = F.normalize(out, p=2)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = self.node_dim\n",
    "        out = self.lin_r(torch_scatter.scatter(\n",
    "            inputs, index, dim=node_dim, reduce='mean'))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph(num_nodes=716847, num_edges=13954819,\n",
    "      ndata_schemes={'feat': Scheme(shape=(300,), dtype=torch.float32), 'label': Scheme(shape=(100,), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
    "      edata_schemes={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "# 使用ObjectView类可以将一个字典的key视作其属性来访问\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关系内部交互层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraAgg(nn.Module):\n",
    "    \"\"\"\n",
    "    在某一关系下进行message aggregate\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dim: int, \n",
    "        output_dim: int, \n",
    "        # features: torch.Tensor, # 怎么可能在你刚初始化的时候就把features传进来呢……你又未必是第一层\n",
    "        # rho: float,\n",
    "        # avg_half_pos_neigh : int, # 用于决定oversample时选多少个同类节点\n",
    "        # train_pos_mask: list,\n",
    "        device: torch.device\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        原文太无赖，居然在pclayer外面就把intraAgg声明好，数据也传进去了 \\\\\n",
    "        :param feature_dim: 原数据每点的特征维数\n",
    "        :param output_dim: 本层的嵌入维度，也就是输出维度\n",
    "        \"\"\"\n",
    "        super(IntraAgg, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "        # self.features = features # (|N|,feat_dim)\n",
    "        # self.rho = rho # 用于距离函数判断的？\n",
    "        self.device = device\n",
    "        # self.train_pos_mask = train_pos_mask # 这是个列表啊\n",
    "        # TODO 为什么这个线性层维度设置怪怪的\n",
    "        self.proj = nn.Linear(2*feature_dim,output_dim)\n",
    "\n",
    "        # 在train阶段，这些rho不会被用，但会被更新\n",
    "        self.rho_neg = 0.5\n",
    "        self.rho_pos = 0.5\n",
    "\n",
    "        # self.avg_half_pos_neigh = avg_half_pos_neigh\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        batch_center_mask: list,\n",
    "        batch_center_labels: list,\n",
    "        train_pos_mask: list,\n",
    "        rx_list: List[list],\n",
    "        batch_center_logits: torch.Tensor, # (|B|,2)\n",
    "        batch_all_logits: torch.Tensor, # (|BatchAll|,2)\n",
    "        train_pos_logits: torch.Tensor, # (|Pos|,2)\n",
    "        trainIdx2OrderIdx: dict,\n",
    "        orderIdx2trainIdx: dict,\n",
    "        avg_half_pos_neigh: int,\n",
    "        train_flag = True # 如果是test，没法靠标签信息来choose\n",
    "    ):\n",
    "        \"\"\"\n",
    "        在一层关系内进行message passing\n",
    "        :param batch_center_mask: 本batch内的中心点\n",
    "        :param batch_center_labels: 本batch中心点的label\n",
    "        :param rx_list: 第x关系的所有中心点的邻居情况 列表套列表\n",
    "        :param batch_center_logits:\n",
    "        :param batch_all_logits:\n",
    "        :param train_pos_logits:\n",
    "        :param trainIdx2orderIdx: 从真正的node id投射到rxlist中索引的词典\n",
    "        \"\"\"\n",
    "        # 此时只是train\n",
    "        self.avg_half_pos_neigh = avg_half_pos_neigh\n",
    "        self.train_pos_mask = train_pos_mask\n",
    "        # 首先，肯定要对邻居进行undersample\n",
    "        # A(v,u)>0 且 D(v,u) < rho-\n",
    "        rx_list_undersampled = []\n",
    "        out_feats = []\n",
    "        for idx, one_center_logits in enumerate(batch_center_logits):\n",
    "            # 先把这个中心点的邻居点的logits提取出来\n",
    "            certain_neighbor_logits = batch_all_logits[rx_list[idx]]\n",
    "            # 计算distance\n",
    "            distance = torch.abs(certain_neighbor_logits - one_center_logits)[:,0]\n",
    "            howManyNeighbors = distance.shape[0]\n",
    "            sampledNeighbor = (distance.argsort()[0:int(howManyNeighbors / 2) + 1]).tolist()\n",
    "            # 对rho-进行更新 这个更新有什么意义吗？\n",
    "            self.rho_neg = distance(distance.argsort()[int(howManyNeighbors / 2)])\n",
    "            # rx_list_undersampled.append(nearest50Idx)\n",
    "            # 这就是我们降采样之后的邻居样本，这里是orderIdx\n",
    "        \n",
    "            # label=1的时候是小样本！\n",
    "            choosedSameClassNode = []\n",
    "            if batch_center_labels[idx] == 1:\n",
    "                # TODO 这里的维度肯定有点问题\n",
    "                distance2 = torch.abs(\n",
    "                    train_pos_logits - one_center_logits)[:, 0]  # 这个时候已经flatten了\n",
    "                choosedSameClassNode = (distance2.argsort()[\n",
    "                    0:self.avg_half_pos_neigh + 1]).tolist()\n",
    "\n",
    "            # undersample之后的orderIdx在undersampledNeighbor里\n",
    "            # oversample之后的orderIdx在choosedSameClassNode里\n",
    "\n",
    "            # 进行aggregate！\n",
    "            neighbor_feats = features[itemgetter(*sampledNeighbor)(orderIdx2trainIdx)]\n",
    "            if not choosedSameClassNode == []:\n",
    "                minor_feats = features[np.array(self.train_pos_mask)[choosedSameClassNode]]\n",
    "                neighbor_feats = torch.cat([neighbor_feats,minor_feats],axis=0)\n",
    "\n",
    "            agg_feats = torch.mean(neighbor_feats,axis=0) \n",
    "\n",
    "            # 把和中心节点的feat进行contact\n",
    "            # 注意有一个reshape！\n",
    "            contacted_feat = torch.cat([features[trainIdx2OrderIdx[idx]],agg_feats],axis=0).reshape(1,-1)\n",
    "            # shape: (1,2*h_{l-1})\n",
    "\n",
    "            # 进行线性映射\n",
    "            out_feats.append(F.relu(self.proj(contacted_feat)))\n",
    "        \n",
    "        rx_out_feats = torch.cat(out_feats,axis=0)\n",
    "        return rx_out_feats\n",
    "\n",
    "\n",
    "    def NeighborhoodSamplerForTraining(\n",
    "        self,\n",
    "        batch_center_logits: torch.Tensor,\n",
    "        batch_center_labels: torch.Tensor,\n",
    "        batch_all_logits: torch.Tensor,\n",
    "        train_pos_logits: torch.Tensor,\n",
    "        rx_list: List[list]\n",
    "        ):\n",
    "        \"\"\"\n",
    "        这里是training阶段，我们将会根据邻居的情况来adaptively决定rho！\n",
    "        \"\"\"\n",
    "        # 首先，肯定要对邻居进行undersample\n",
    "        # A(v,u)>0 且 D(v,u) < rho-\n",
    "        rx_list_undersampled = []\n",
    "        for idx, one_center_logits in enumerate(batch_center_logits):\n",
    "            # 先把这个中心点的邻居点的logits提取出来\n",
    "            certain_neighbor_logits = batch_all_logits[rx_list[idx]]\n",
    "            # 进行相减！\n",
    "            distance = torch.abs(certain_neighbor_logits - one_center_logits)[:,0]\n",
    "            howManyNeighbors = distance.shape[0]\n",
    "            undersampledNeighbor = (distance.argsort()[0:int(howManyNeighbors / 2) + 1]).tolist()\n",
    "            # 对rho-进行更新 这个更新有什么意义吗？\n",
    "            self.rho_neg = distance(distance.argsort()[int(howManyNeighbors / 2)])\n",
    "            # rx_list_undersampled.append(nearest50Idx)\n",
    "            # 这就是我们降采样之后的邻居样本，这里是orderIdx\n",
    "        \n",
    "            # label=1的时候是小样本！\n",
    "            if batch_center_labels[idx] == 1:\n",
    "                # TODO 这里的维度肯定有点问题\n",
    "                distance2 = torch.abs(train_pos_logits - one_center_logits)[:,0] # 这个时候已经flatten了\n",
    "                choosedSameClassNode = distance2.argsort()[0:self.avg_half_pos_neigh + 1]\n",
    "\n",
    "            # undersample之后的orderIdx在undersampledNeighbor里\n",
    "            # oversample之后的orderIdx在choosedSameClassNode里\n",
    "\n",
    "        \n",
    "\n",
    "    def NeighborhoodSamplerForTest(\n",
    "        self,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三层关系交互层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterAgg(nn.Module):\n",
    "    \"\"\" \n",
    "    对三层关系进行message aggregate\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # features: torch.Tensor,\n",
    "        feature_dim: int,\n",
    "        output_dim: int,\n",
    "        # adj_lists: defaultdict,\n",
    "        # train_pos_mask: list,\n",
    "        device: torch.device,\n",
    "        num_classes:int = 2,\n",
    "        num_relations:int =3\n",
    "        ) -> None:\n",
    "        super(InterAgg, self).__init__()\n",
    "        # self.features = features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "        # self.adj_lists = adj_lists # 3个关系的defaultdict\n",
    "        # self.train_pos_mask = train_pos_mask\n",
    "        self.device = device\n",
    "        self.num_classes =  num_classes\n",
    "\n",
    "        # 三个关系的embedding要糅合成一个，需要进行一个线性层转换\n",
    "        self.proj = nn.Linear(\n",
    "            in_features=num_relations*self.output_dim + self.feature_dim,\n",
    "            # 入维度是三个realtion的embedding和原特征contact在一起的\n",
    "            out_features=output_dim\n",
    "        )\n",
    "\n",
    "        # 可是距离函数不是每个关系的都不一样吗？？\n",
    "        self.label_linear = nn.Linear(self.feature_dim,self.num_classes)\n",
    "        # 这个距离函数甚至只输出logits，没有进行sigmoid\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # 准备intraAgg层\n",
    "        self.intra1 = IntraAgg(\n",
    "            feature_dim=self.feature_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            # avg_half_pos_neigh=self.avg_half_neigh_size[0],\n",
    "            # train_pos_mask=self.train_pos_mask,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.intra2 = IntraAgg(\n",
    "            self.feature_dim,self.output_dim,self.device\n",
    "        )\n",
    "        self.intra3 = IntraAgg(\n",
    "            self.feature_dim,self.output_dim,self.device\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "        batch_center_mask,\n",
    "        batch_center_label,\n",
    "        train_pos_mask,\n",
    "        adj_lists,\n",
    "        train_flag = True\n",
    "        ):\n",
    "        \"\"\" \n",
    "        :param batch_mask: 本批次中要训练的点\n",
    "        :param batch_label: 本批次中训练点的label\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.adj_lists = adj_lists\n",
    "        self.train_pos_mask = train_pos_mask\n",
    "\n",
    "        # 计算minority class的average neighborhood size\n",
    "        avg_half_neigh_size = []\n",
    "        for relationIdx in range(len(self.adj_lists)):\n",
    "            total = 0\n",
    "            for trainIdx in self.train_pos_mask:\n",
    "                total += len(self.adj_lists[relationIdx][trainIdx])\n",
    "            avg_half_neigh_size.append(total / len(self.train_pos_mask))\n",
    "        self.avg_half_neigh_size = avg_half_neigh_size\n",
    "\n",
    "\n",
    "        # batch_mask是什么，是本batch中所要考察的中心点\n",
    "        # 我们后续要用到的信息包括本batch的中心点及它们的1-hop neighbor\n",
    "        # 所以搞一个batch_all_mask，就包括了上述这些需要的点    \n",
    "        to_neighs = []  # to_neighs里面最终将会是三个列表，每个列表是某一个关系的adjlist转成列表\n",
    "        for adj_list in self.adj_lists:\n",
    "            to_neighs.append([set(adj_list[int(node)]) for node in batch_center_mask])\n",
    "        # to_neighs be like: [[{某点的所有邻居},{},...,{}],[],[]]\n",
    "        batch_all_nodes = set.union(set.union(*to_neighs[0]), set.union(*to_neighs[1]),\n",
    "                                 set.union(*to_neighs[2], set(batch_center_mask)))\n",
    "        # batch_all_nodes be like: {0,1,3,4,5,...}\n",
    "        batch_all_mask = list(batch_all_nodes)\n",
    "        # batch_all_mask be like: [0,1,3,4,5,...]\n",
    "        # batch_all_mask 内承载了本batch训练所需的所有点的index -> TODO 这个index是针对谁来说的？\n",
    "        \n",
    "        # 提取出本batch all点的feature\n",
    "        # batch_features = self.features[batch_all_mask]\n",
    "        # 我去，features是Embedding层……\n",
    "        batch_all_features = self.features[torch.LongTensor(batch_all_mask).to(self.device)]\n",
    "        # -> shape (|BatchAll|,feat_dim)\n",
    "        # postive nodes features\n",
    "        pos_features = self.feautres[torch.LongTensor(self.train_pos_mask).to(self.device)]\n",
    "        # -> shape (|Pos|,feat_dim)\n",
    "\n",
    "        # 出于加快访问速度的考虑（应该是），defaultdict涉及到查找过程——这太慢啦！\n",
    "        # 但是nodes的trainIdx和在batch_all_mask中的orderIdx需要进行相互转化，对吧\n",
    "        trainIdx2orderIdx = {trainIdx : orderIdx for trainIdx, orderIdx in zip(batch_all_nodes, range(len(batch_all_nodes)))}\n",
    "        orderIdx2trainIdx = (lambda d: dict(zip(d.itervalues(),d.iterkeys())))(trainIdx2orderIdx)\n",
    "\n",
    "        # TODO 可是score不是在每个intra层里单独算的吗？？？\n",
    "        # 先把batch all的logits都算完\n",
    "        batch_all_logits = self.label_linear(batch_all_features)\n",
    "        # 注意到，pos mask中的点很明显可能不在batch all中\n",
    "        pos_logits = self.label_linear(pos_features)\n",
    "\n",
    "        # 提取一些特定点的logits\n",
    "        # 提取本batch center点的logits\n",
    "        batch_center_logits = batch_all_logits[itemgetter(*batch_center_mask)(trainIdx2orderIdx)]\n",
    "\n",
    "        r1_list = [list(to_neigh) for to_neigh in to_neighs[0]]\n",
    "        r2_list = [list(to_neigh) for to_neigh in to_neighs[1]]\n",
    "        r3_list = [list(to_neigh) for to_neigh in to_neighs[2]]\n",
    "        # rx_list: [[此关系下某个点的所有邻居],[],[]...,[]]\n",
    "\n",
    "        r1_embeds = self.intra1.forward(\n",
    "            batch_center_mask,\n",
    "            batch_center_label,\n",
    "            self.train_pos_mask,\n",
    "            r1_list,\n",
    "            batch_center_logits,\n",
    "            batch_all_logits,\n",
    "            pos_logits,\n",
    "            trainIdx2orderIdx,\n",
    "            orderIdx2trainIdx,\n",
    "            self.avg_half_neigh_size[0]\n",
    "        )\n",
    "        r2_embeds = self.intra1.forward(\n",
    "            batch_center_mask,\n",
    "            batch_center_label,            \n",
    "            self.train_pos_mask,            \n",
    "            r2_list,\n",
    "            batch_center_logits,\n",
    "            batch_all_logits,\n",
    "            pos_logits,\n",
    "            trainIdx2orderIdx,\n",
    "            orderIdx2trainIdx,\n",
    "            self.avg_half_neigh_size[1]\n",
    "        )\n",
    "        r3_embeds = self.intra1.forward(\n",
    "            batch_center_mask,\n",
    "            batch_center_label,\n",
    "            self.train_pos_mask,\n",
    "            r3_list,\n",
    "            batch_center_logits,\n",
    "            batch_all_logits,\n",
    "            pos_logits,\n",
    "            trainIdx2orderIdx,\n",
    "            orderIdx2trainIdx,\n",
    "            self.avg_half_neigh_size[2]\n",
    "        )\n",
    "        # rx_embeds -> (|B|,out_dim)\n",
    "        all_relation_and_self_embeds = torch.cat([self.features[batch_center_mask],r1_embeds,r2_embeds,r3_embeds],dim=1)\n",
    "        proj_all_embeds = F.relu(self.proj(all_relation_and_self_embeds))\n",
    "\n",
    "        return proj_all_embeds, batch_center_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC-GNN消息传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCGNN(nn.Module):\n",
    "    \"\"\" \n",
    "    一层PC-GNN用以message passing -> 核心特点是，居然需要label=。= \\\\\n",
    "    论文源代码居然直接把一层当整个模型了……好🐕啊\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int,\n",
    "        # adj_lists: List[defaultdict],\n",
    "        # train_pos_mask: list,\n",
    "        device: torch.device,\n",
    "        normalize = True,\n",
    "        num_classes: int = 2,\n",
    "        bias = False, \n",
    "    ):  \n",
    "        \"\"\"\n",
    "        一层Pick&Choose 的 message passager \\\\\n",
    "        难道inter层不应该在这里进行声明吗…… \\\\\n",
    "        :param in_channels: 输入的特征维数\n",
    "        :param out_channels: 输出的特征维数\n",
    "        :param num_classes: 最后需要做节点分类的类数\n",
    "        \"\"\"\n",
    "        super(PCGNN, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.interAgg = InterAgg(\n",
    "            feature_dim=in_channels,\n",
    "            output_dim=out_channels,\n",
    "            # adj_lists=adj_lists,\n",
    "            # train_pos_mask=train_pos_mask,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # self.lin_l.reset_parameters()\n",
    "        # self.lin_r.reset_parameters()\n",
    "        pass\n",
    "\n",
    "    def forward(self, features ,labels, batch_mask, train_pos_mask, adj_lists, train_flag = True):\n",
    "        \"\"\"\n",
    "        :param features: (|N|, input_channels)\n",
    "        :param labels: (|N|,)\n",
    "        :param batch_mask: (|B|,)在此次过程中需要考察的中心点的mask\n",
    "        :param train_pos_mask:\n",
    "        :param adj_lists:\n",
    "        :return output_embeds: (|B|,out_dim)\n",
    "        :return label_scores: (|B|,2)\n",
    "        \"\"\"\n",
    "\n",
    "        embeds, logits = self.interAgg(\n",
    "            features=features,\n",
    "            batch_center_mask=batch_mask,\n",
    "            batch_center_label=labels[batch_mask],\n",
    "            train_pos_mask=train_pos_mask,\n",
    "            adj_lists=adj_lists\n",
    "        )\n",
    "\n",
    "        return embeds, logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个args实例：\n",
    "\n",
    "{'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32,\n",
    "        'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNNStack 范式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim : int, \n",
    "        hidden_dim: int,\n",
    "        output_dim : int,\n",
    "        device: torch.device,\n",
    "        num_classes : int = 2,\n",
    "        num_layers : int = 1,\n",
    "        dropout : float = 0.5, \n",
    "        heads : int = 1,\n",
    "        model_type : str = 'PCGNN',\n",
    "        emb : bool = False\n",
    "        ) -> None:\n",
    "        super(GNNStack, self).__init__()\n",
    "\n",
    "        conv_model = self.build_conv_model(model_type)\n",
    "        # self.convs = nn.ModuleList()\n",
    "        # self.convs.append(conv_model(\n",
    "        #     in_channels=input_dim,\n",
    "        #     out_channels=hidden_dim\n",
    "        # ))\n",
    "        assert (num_layers >= 1), 'Number of layers is not >=1'\n",
    "        if num_layers == 1:\n",
    "            self.convs = conv_model(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=output_dim,\n",
    "                device=device\n",
    "            )\n",
    "        else: \n",
    "            # for l in range(num_layers):\n",
    "            #     if l == 0:\n",
    "            #         self.convs.append(conv_model(input_dim,hidden_dim))\n",
    "            #     elif l == num_layers-1:\n",
    "            #         self.convs.append(conv_model(heads*hidden_dim,output_dim))\n",
    "            #     else:\n",
    "            #         self.convs.append(conv_model(heads*hidden_dim, hidden_dim))\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # # post-message-passing\n",
    "        # self.post_mp = nn.Sequential(\n",
    "        #     nn.Linear(heads * hidden_dim, hidden_dim), nn.Dropout(self.dropout), \n",
    "        #     nn.Linear(hidden_dim, output_dim))\n",
    "        # # 嗯……其实我一共设了num_layers+1层\n",
    "\n",
    "        # PCGNN最后还有一个线性层用于node分类！\n",
    "        self.final_proj = nn.Linear(output_dim,num_classes)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'PCGNN':\n",
    "            return PCGNN\n",
    "        else: \n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, features, labels, batch_mask, train_pos_mask, adj_lists):\n",
    "        \"\"\" \n",
    "        :param features: 所有点的features\n",
    "        :param labels: 所有点的label\n",
    "        :param batch_mask: 本批次点的mask 或者是之后test/valid的时候的mask\n",
    "        :param train_pos_mask:\n",
    "        :param adj_lists:\n",
    "        \"\"\"\n",
    "        # x, edge_index, batch = data.x, data.edge_index, data.batch        \n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            embeds, logits = self.convs(features,labels,batch_mask,train_pos_mask,adj_lists)\n",
    "            # x = F.relu(x)\n",
    "            # x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        # x = self.post_mp(x)\n",
    "        if self.emb == True:\n",
    "            return embeds\n",
    "\n",
    "        return embeds, logits\n",
    "\n",
    "    def loss(self, features, labels, batch_mask, train_pos_mask, adj_lists):\n",
    "        \"\"\" \n",
    "        PCGNN的loss包括两个:loss_{gnn}和loss_{dist}\n",
    "        \"\"\"\n",
    "        embeds,logits = self.forward(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            batch_mask=batch_mask,\n",
    "            train_pos_mask=train_pos_mask,\n",
    "            adj_lists=adj_lists            \n",
    "        )\n",
    "\n",
    "        # PCGNN有两个loss\n",
    "        # loss_{gnn}\n",
    "        gnn_pred = self.final_proj(embeds)\n",
    "        gnn_loss = self.criterion(gnn_pred,labels[batch_mask].squeeze())\n",
    "\n",
    "        # loss_{dist}\n",
    "        dist_loss = self.criterion(logits,labels[batch_mask].squeeze())\n",
    "\n",
    "        return gnn_loss + dist_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p: p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr,\n",
    "                               weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr,\n",
    "                              momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(\n",
    "            filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(\n",
    "            filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个args实例：\n",
    "\n",
    "{'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32,\n",
    "        'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HeteroData(\n",
      "  homo=[11944],\n",
      "  \u001b[1mreview\u001b[0m={\n",
      "    x=[11944, 25],\n",
      "    y=[11944],\n",
      "    train_mask=[3455],\n",
      "    valid_mask=[1710],\n",
      "    test_mask=[3474]\n",
      "  },\n",
      "  \u001b[1m(review, r1, review)\u001b[0m={\n",
      "    adj=[11944, 11944],\n",
      "    adj_list=[11944],\n",
      "    edge_index=[2, 351216]\n",
      "  },\n",
      "  \u001b[1m(review, r2, review)\u001b[0m={\n",
      "    adj=[11944, 11944],\n",
      "    adj_list=[11944],\n",
      "    edge_index=[2, 7132958]\n",
      "  },\n",
      "  \u001b[1m(review, r3, review)\u001b[0m={\n",
      "    adj=[11944, 11944],\n",
      "    adj_list=[11944],\n",
      "    edge_index=[2, 2073474]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def show_data(dataset_name:str = 'Amazon'):\n",
    "    # print(f\"Node Classification. test set size: {graph.ndata['train_mask'].sum().item()}\")\n",
    "    print()\n",
    "\n",
    "    data = constructPyGHeteroData(dataset_name)\n",
    "    # data是一个heterogeneous图，一种节点，三种关系\n",
    "    print(data)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCGNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: HeteroData,\n",
    "        data_name:str = 'Amazon',\n",
    "        random_seed:int = 42,\n",
    "        use_cuda:bool = True,\n",
    "        opt:str = 'adam',\n",
    "        weight_decay:float = 5e-3,\n",
    "        lr:float = 0.01,\n",
    "        dropout:float = 0.5,\n",
    "        num_layers:int = 2,\n",
    "        num_epochs:int = 50,\n",
    "        batch_size:int = 256\n",
    "        ) -> None:\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "\n",
    "        # preprare data\n",
    "        self.data = data\n",
    "        # 关于此数据集有个很重要的事，即，node数据都是ndarray格式，relation的数据都是tensor！\n",
    "        # TODO normlize feature??\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.opt, self.weight_decay,self.lr = opt,weight_decay,lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        feat_data,label_data = self.data['review'].x,self.data['review'].y\n",
    "        train_mask,valid_mask,test_mask = self.data['review'].train_mask,self.data['review'].valid_mask,self.data['review'].test_mask\n",
    "        train_pos_mask = self.data['review'].train_pos_mask\n",
    "        adj_lists = [\n",
    "            self.data['review','r1','review'].adj_list[0],\n",
    "            self.data['review','r2','review'].adj_list[0],\n",
    "            self.data['review','r3','review'].adj_list[0],\n",
    "        ]\n",
    "\n",
    "        features = nn.Embedding(feat_data.shape[0], feat_data.shape[1])\n",
    "        features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False).to(self.device)\n",
    "\n",
    "        # 我们只使用PCGNN\n",
    "        GNN = GNNStack(\n",
    "            input_dim=feat_data.shape[1],\n",
    "            hidden_dim=64,\n",
    "            output_dim=64,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # optimizer\n",
    "        if(self.opt == 'adam'):\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, GNN.parameters()), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError(\"This optimizer is not implemented yet.\")\n",
    "        \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            # Pick阶段，借助labelBalancedSampler进行一个降采样\n",
    "            train_mask = LabelBalancedSampler(train_mask,label_data[train_mask],self.data['homo_adj_list'][0],)\n",
    "            # 这里的train mask是经过概率pick过的！\n",
    "\n",
    "            # 我们在准备数据集时已经准备好了train_pos/neg_mask\n",
    "            num_batches = int(len(train_mask) / self.batch_size) + 1\n",
    "\n",
    "            loss = 0.\n",
    "\n",
    "            # 开始batch训练\n",
    "            for batch in range(num_batches):\n",
    "                ind_start = batch*self.batch_size\n",
    "                ind_end = min(batch*self.batch_size,len(train_mask))\n",
    "                batch_nodes_mask = train_mask[ind_start:ind_end]\n",
    "                # batch_label = label_data[batch_nodes_mask]\n",
    "                # TODO 这里的类型可能存在问题\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = GNN.loss(\n",
    "                    features=feat_data,\n",
    "                    labels=label_data,\n",
    "                    batch_mask=batch_nodes_mask,\n",
    "                    train_pos_mask=train_pos_mask,\n",
    "                    adj_lists=adj_lists\n",
    "                )\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss += loss.item()\n",
    "\n",
    "                print(f'Epoch: {epoch}, loss: {loss.item() / num_batches}')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始漫漫debug……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import models\n",
    "from myutils import constructPyGHeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = constructPyGHeteroData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0, len(rx_list[idx])=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\PC_GNN.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Univ/dl-papers/PC-GNN-reproduction/PC_GNN.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mModelHandler(data)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Univ/dl-papers/PC-GNN-reproduction/PC_GNN.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\models.py:275\u001b[0m, in \u001b[0;36mModelHandler.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39m# batch_label = label_data[batch_nodes_mask]\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m# TODO 这里的类型可能存在问题\u001b[39;00m\n\u001b[0;32m    274\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 275\u001b[0m loss \u001b[39m=\u001b[39m GNN\u001b[39m.\u001b[39;49mloss(\n\u001b[0;32m    276\u001b[0m     features\u001b[39m=\u001b[39;49mfeat_data,\n\u001b[0;32m    277\u001b[0m     labels\u001b[39m=\u001b[39;49mlabel_data,\n\u001b[0;32m    278\u001b[0m     batch_mask\u001b[39m=\u001b[39;49mbatch_nodes_mask,\n\u001b[0;32m    279\u001b[0m     train_pos_mask\u001b[39m=\u001b[39;49mtrain_pos_mask,\n\u001b[0;32m    280\u001b[0m     adj_lists\u001b[39m=\u001b[39;49madj_lists\n\u001b[0;32m    281\u001b[0m )\n\u001b[0;32m    283\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    284\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\models.py:170\u001b[0m, in \u001b[0;36mGNNStack.loss\u001b[1;34m(self, features, labels, batch_mask, train_pos_mask, adj_lists)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, features, labels, batch_mask, train_pos_mask, adj_lists):\n\u001b[0;32m    167\u001b[0m     \u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m    PCGNN的loss包括两个:loss_{gnn}和loss_{dist}\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     embeds, logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[0;32m    171\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    172\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m    173\u001b[0m         batch_mask\u001b[39m=\u001b[39;49mbatch_mask,\n\u001b[0;32m    174\u001b[0m         train_pos_mask\u001b[39m=\u001b[39;49mtrain_pos_mask,\n\u001b[0;32m    175\u001b[0m         adj_lists\u001b[39m=\u001b[39;49madj_lists\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[39m# PCGNN有两个loss\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     \u001b[39m# loss_{gnn}\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     gnn_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_proj(embeds)\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\models.py:155\u001b[0m, in \u001b[0;36mGNNStack.forward\u001b[1;34m(self, features, labels, batch_mask, train_pos_mask, adj_lists)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m# x, edge_index, batch = data.x, data.edge_index, data.batch\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[1;32m--> 155\u001b[0m     embeds, logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs(\n\u001b[0;32m    156\u001b[0m         features, labels, batch_mask, train_pos_mask, adj_lists)\n\u001b[0;32m    157\u001b[0m     \u001b[39m# x = F.relu(x)\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# x = F.dropout(x, p=self.dropout,training=self.training)\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[39m# x = self.post_mp(x)\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\models.py:73\u001b[0m, in \u001b[0;36mPCGNN.forward\u001b[1;34m(self, features, labels, batch_mask, train_pos_mask, adj_lists, train_flag)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, features, labels, batch_mask, train_pos_mask, adj_lists, train_flag\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    :param features: (|N|, input_channels)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    :param labels: (|N|,)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :return label_scores: (|B|,2)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     embeds, logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterAgg(\n\u001b[0;32m     74\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m     75\u001b[0m         batch_center_mask\u001b[39m=\u001b[39;49mbatch_mask,\n\u001b[0;32m     76\u001b[0m         batch_center_label\u001b[39m=\u001b[39;49mlabels[batch_mask],\n\u001b[0;32m     77\u001b[0m         train_pos_mask\u001b[39m=\u001b[39;49mtrain_pos_mask,\n\u001b[0;32m     78\u001b[0m         adj_lists\u001b[39m=\u001b[39;49madj_lists\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m embeds, logits\n",
      "File \u001b[1;32mc:\\Users\\bin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\agg.py:305\u001b[0m, in \u001b[0;36mInterAgg.forward\u001b[1;34m(self, features, batch_center_mask, batch_center_label, train_pos_mask, adj_lists, train_flag)\u001b[0m\n\u001b[0;32m    302\u001b[0m r3_list \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39m(to_neigh) \u001b[39mfor\u001b[39;00m to_neigh \u001b[39min\u001b[39;00m to_neighs[\u001b[39m2\u001b[39m]]\n\u001b[0;32m    303\u001b[0m \u001b[39m# rx_list: [[此关系下某个点的所有邻居],[],[]...,[]]\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m r1_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintra1\u001b[39m.\u001b[39;49mforward(\n\u001b[0;32m    306\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures,\n\u001b[0;32m    307\u001b[0m     batch_center_mask,\n\u001b[0;32m    308\u001b[0m     batch_center_label,\n\u001b[0;32m    309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_pos_mask,\n\u001b[0;32m    310\u001b[0m     r1_list,\n\u001b[0;32m    311\u001b[0m     batch_center_logits,\n\u001b[0;32m    312\u001b[0m     batch_all_logits,\n\u001b[0;32m    313\u001b[0m     pos_logits,\n\u001b[0;32m    314\u001b[0m     trainIdx2orderIdx,\n\u001b[0;32m    315\u001b[0m     orderIdx2trainIdx,\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mavg_half_neigh_size[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    317\u001b[0m )\n\u001b[0;32m    318\u001b[0m r2_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintra1\u001b[39m.\u001b[39mforward(\n\u001b[0;32m    319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures,\n\u001b[0;32m    320\u001b[0m     batch_center_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavg_half_neigh_size[\u001b[39m1\u001b[39m]\n\u001b[0;32m    330\u001b[0m )\n\u001b[0;32m    331\u001b[0m r3_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintra1\u001b[39m.\u001b[39mforward(\n\u001b[0;32m    332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures,\n\u001b[0;32m    333\u001b[0m     batch_center_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavg_half_neigh_size[\u001b[39m2\u001b[39m]\n\u001b[0;32m    343\u001b[0m )\n",
      "File \u001b[1;32md:\\Univ\\dl-papers\\PC-GNN-reproduction\\agg.py:124\u001b[0m, in \u001b[0;36mIntraAgg.forward\u001b[1;34m(self, features, batch_center_mask, batch_center_labels, train_pos_mask, rx_list, batch_center_logits, batch_all_logits, train_pos_logits, trainIdx2OrderIdx, orderIdx2trainIdx, avg_half_pos_neigh, train_flag)\u001b[0m\n\u001b[0;32m    119\u001b[0m agg_feats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(neighbor_feats, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[39m# 把和中心节点的feat进行contact\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m# 注意有一个reshape！\u001b[39;00m\n\u001b[0;32m    123\u001b[0m contacted_feat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m--> 124\u001b[0m     [features[trainIdx2OrderIdx[idx]], agg_feats], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[39m# shape: (1,2*h_{l-1})\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[39m# 进行线性映射\u001b[39;00m\n\u001b[0;32m    128\u001b[0m out_feats\u001b[39m.\u001b[39mappend(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(contacted_feat)))\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model = models.ModelHandler(data)\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b03cd7f13856fb3b521b8bb7c9e86e32af68550ba3f54aa9b7b683fd8765685a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
